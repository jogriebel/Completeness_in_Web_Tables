{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nearby-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import random\n",
    "import py7zr\n",
    "import glob\n",
    "import re\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed250a3-332b-43f1-98f8-3d92b20e0c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ultimate_filter(unfilteredInputfile_path, outpath):    \n",
    "    \n",
    "    page_object=[] #wird eine Liste von dicts\n",
    "    result=list() #Sammelliste für alle Tabellen mit min. 2 row change\n",
    "       with open(unfilteredInputfile_path, 'r', encoding=\"utf8\") as f:\n",
    "        data_prev = {}\n",
    "        current=json.loads(next(f))\n",
    "        for line in f:\n",
    "\n",
    "            data=json.loads(line)\n",
    "            if data_prev =={} : #Beim allerersten Durchgang\n",
    "               # print(\"Aller Anfang\")\n",
    "                page_object = []\n",
    "                page_object.append(current) #beim ersten Durchgang muss das mit dem current dict erstellt werden\n",
    "                # alles mit der gleichen pageID in ein Objekt zusammenpacken\n",
    "            elif current[\"pageID\"] != data_prev[\"pageID\"] and current[\"pageID\"]==data[\"pageID\"]:\n",
    "                #print(\"NEW PAGE IS BEGINNING\")\n",
    "                #empty the page object and start over to fill it\n",
    "                page_object = []\n",
    "                page_object.append(current)\n",
    "\n",
    "            elif current[\"pageID\"] == data_prev[\"pageID\"] and current[\"pageID\"]==data[\"pageID\"]:#wenn die Seite gleichbleibt\n",
    "                #print(\"same old\")\n",
    "                page_object.append(data)\n",
    "\n",
    "            elif current[\"pageID\"] == data_prev[\"pageID\"] and current[\"pageID\"]!=data[\"pageID\"]: #Seite wird sich ändern\n",
    "                #object for this page is now complete\n",
    "                #print(\"Page is about to change\")\n",
    "                #filter tables within that page\n",
    "                #filter for list/dict per table\n",
    "                #sort collected tables by key\n",
    "                page_object=sorted(page_object, key=lambda d: d['key'])\n",
    "                key_prev=page_object[0][\"key\"]#am Anfang ist der vorherige Key gleich dem aktuellen der Einfachheit halber\n",
    "                tab_object = []\n",
    "                for tab in range(len(page_object)):\n",
    "\n",
    "                    if page_object[tab][\"key\"]== key_prev:\n",
    "                        tab_object.append(page_object[tab])\n",
    "                        #print(\"Type: \", page_object[tab][\"contentType\"], \"tab0\")\n",
    "                    else:\n",
    "                        #before creating a new tab_object, process the current one\n",
    "                        #alle dicts, die keine rows haben, bekommen 0 zugewiesen\n",
    "                        for i in range(len(tab_object)):\n",
    "                            tab_object[i][\"rows\"]=tab_object[i].get(\"rows\",0)\n",
    "                        #sort by key \n",
    "                        seq=[x['rows'] for x in tab_object]\n",
    "                        \n",
    "                        \n",
    "                        if len(tab_object)>2 and max(seq)>2 and tab_object[len(tab_object)-1][\"contentType\"]!=\"DELETE\":#Tabellen, die nur einmal auftreten ausschließen und solche mit zwei oder weniger Zeilen\n",
    "                            if len(tab_object)<3:\n",
    "                                print(len(tab_object), \"ZU WENIGE ENTRIES!!!!!!!!\", tab_object[len(tab_object)-1][\"key\"])\n",
    "                            #print(\"Number of rows in table: \")\n",
    "                            if max(seq)<3:\n",
    "                                print(max(seq), \"F...., die rows\")\n",
    "                            \n",
    "                            if tab_object[len(tab_object)-1][\"contentType\"]==\"DELETE\":\n",
    "                                print(tab_object[len(tab_object)-1][\"key\"], \"SHIT\")\n",
    "                            \n",
    "                            \n",
    "                            #filter within table objects:remove if too short distance between edits or UNMODIFIED\n",
    "                            tab_object=sorted(tab_object, key=lambda d: d['validFrom'])\n",
    "                            filtered_versions=[]\n",
    "                            prev_version = tab_object.index(tab_object[0])  # am Anfang ist der vorherige Key gleich dem aktuellen der Einfachheit halber\n",
    "                            for version in range(len(tab_object)):\n",
    "                                #distance too short, remove\n",
    "                                if (abs(datetime.strptime(tab_object[prev_version][\"validFrom\"], '%Y-%m-%dT%H:%M:%SZ') - datetime.strptime(tab_object[version][\"validFrom\"], '%Y-%m-%dT%H:%M:%SZ')).days > 0\\\n",
    "                                        and tab_object[version][\"contentType\"]!=\"UNMODIFIED\" and tab_object[version][\"contentType\"]!=\"DELETE\" ) or tab_object[version][\"contentType\"]==\"CREATE\":\n",
    "                                    # or CREATE  is important to keepthe create element of that table (comparsion in the first iteration with itself)\n",
    "                                    #print(\"Nice Version detected\")\n",
    "                                    filtered_versions.append(tab_object[version])\n",
    "                                    prev_version=tab_object.index(tab_object[version])\n",
    "\n",
    "                            filtered_versions = sorted(filtered_versions, key=lambda d: (d['key'],d['validFrom']))\n",
    "                            if len(filtered_versions)>2 and filtered_versions[len(filtered_versions)-1][\"rows\"]>0:#make sure after all that filtering still enough entries remain for that table\n",
    "                                if filtered_versions[len(filtered_versions)-1][\"rows\"]<1:\n",
    "                                    print(\"LastRows: \",filtered_versions[len(filtered_versions)-1][\"rows\"])\n",
    "                                #and the table should have more than 0 rows\n",
    "                                \n",
    "                                filename=outpath + tab_object[0]['key'] +'.json'\n",
    "                                #print(filename)\n",
    "                               \n",
    "                                with open(filename, 'w') as fp:\n",
    "                                    json.dump(filtered_versions, fp)#tab_object\n",
    "\n",
    "                            \n",
    "                        #print(\"new tab object created\")\n",
    "                        tab_object=[]\n",
    "                        tab_object.append(page_object[tab])\n",
    "                    key_prev=page_object[tab][\"key\"]\n",
    "                    #print(key_prev)\n",
    "\n",
    "\n",
    "\n",
    "            else: # if nothig of the above is applicable\n",
    "                #print(\"ELSE\")\n",
    "                page_object={}\n",
    "                page_object.update(data)\n",
    "            #reset prev, current and next\n",
    "            data_prev=current\n",
    "            current=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd0b6e-3322-4d12-bcba-85ca6af721c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all json files into one\n",
    "def collect_filtered_files(inpath, pagefilename ):\n",
    "    result=list() #Sammelliste für alle Tabellen mit min. 2 row change\n",
    "    jsonfiles=[f for f in os.listdir(inpath) if f.endswith('.json')]\n",
    "    os.chdir(inpath)\n",
    "    for f1 in jsonfiles:\n",
    "        with open(f1, 'r') as infile:\n",
    "            result.extend(json.load(infile))\n",
    "    with open(pagefilename, 'w') as output_file:\n",
    "        json.dump(result, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3b5fbd-3a82-4834-ac90-52e0caa21a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_filtered_json(json_inpath):    \n",
    "\n",
    "    with open(json_inpath) as json_file:\n",
    "        data=json.load(json_file)\n",
    "\n",
    "    #convert the json-file to a dataframe\n",
    "    tabledf=pd.DataFrame.from_dict(data, orient=\"columns\")\n",
    "    return tabledf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f960ad-b63f-4426-b023-cb4ee2da739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_page(tabledf_page):\n",
    "    #drop unneccessary columns\n",
    "    tabledf_page=tabledf_page.drop(columns=['pageID', 'contentHash', 'revisionId','similarityLast',  'comment', 'position','user', 'caption'])\n",
    "    \n",
    "    #ValidFrom with datetime format\n",
    "    tabledf_page[\"validFrom\"]=pd.to_datetime(tabledf_page[\"validFrom\"], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "    tabledf_page[\"timestamp_c\"]=tabledf_page.validFrom.transform(lambda x: x-x.min())\n",
    "    \n",
    "    #generate seperate df for each table on that page\n",
    "    grouped=tabledf_page.groupby(\"key\")\n",
    "    result=[g[1]for g in list(grouped)[:]]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brutal-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get the files in the order displayed in the explorer ###\n",
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] \n",
    "    return sorted(data, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bebf4-3056-4ce9-8789-43962ea37149",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "Download matched tables from https://owncloud.hpi.de/s/oLQ7zrblMJHvNqd and save the files in a directory called \"matched-tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "furnished-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "###rename table files: prepend number (for the selection procedure later on)\n",
    "path=\"your-absolute-path-to-matched-tables\"\n",
    "files = sorted_alphanumeric(os.listdir(path))\n",
    "\n",
    "for index, file in enumerate(files):\n",
    "    os.rename(os.path.join(path, file), os.path.join(path, ''.join([str(index),\"_\",file])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "yellow-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create array with random numbers to select random buckets###\n",
    "random.seed(12)\n",
    "random_buckets=random.sample(range(656), 25)\n",
    "random_buckets=sorted(random_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "skilled-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample(begin, end, random_buckets, files, path, pklname, tabprobucket):  \n",
    "    rs_list=[]\n",
    "    rs_listlist=[]\n",
    "    ### extract 7z file ###\n",
    "    os.chdir(path)\n",
    "    extract_path=path\n",
    "    os.chdir(\"..\")\n",
    "    outpath_zip=(os.path.join(os.path.abspath(os.curdir), str(\"output-table\")))\n",
    "    outpath_filter=(os.path.join(os.path.abspath(os.curdir), str(\"out_filter\")))\n",
    "    for index in range(begin,end):\n",
    "        print(random_buckets[index])\n",
    "        collected_name=\"result_bucket_\"+str(random_buckets[index])+\".json\"\n",
    "        if os.path.exists(os.path.join(outpath_filter, str(random_buckets[index]), collected_name)):\n",
    "            out_filterb=(os.path.join(outpath_filter, str(random_buckets[index])+\"\\ \"))[:-1]\n",
    "            print(random_buckets[index], \": exists\")\n",
    "        else:    \n",
    "\n",
    "\n",
    "            #unzip bucket\n",
    "            print(\"unzipping...\")\n",
    "            with py7zr.SevenZipFile(os.path.join(path, files[random_buckets[index]]), mode='r') as z:\n",
    "                z.extractall(path=outpath_zip)\n",
    "\n",
    "            name=re.sub(r'^.*?enwiki', 'enwiki',os.path.splitext(files[random_buckets[index]])[0])\n",
    "            print(name)\n",
    "           \n",
    "            ##create directory to collect single tables from filtering\n",
    "            if not os.path.exists(os.path.join(outpath_filter, str(random_buckets[index]))):\n",
    "                os.mkdir(os.path.join((outpath_filter), str(random_buckets[index])+\"\\ \"))\n",
    "                out_filterb=os.path.join(outpath_filter, str(random_buckets[index])+\"\\ \")\n",
    "            else:\n",
    "                out_filterb=os.path.join(outpath_filter, str(random_buckets[index])+\"\\ \")\n",
    "\n",
    "            #filter bucket   \n",
    "            print(\"filter bucket...\")\n",
    "            ultimate_filter(os.path.join(outpath_zip,\"output-table\", name ), out_filterb) #os.path.join(outpath_zip,\"output-table\", name )\n",
    "\n",
    "            #collect filtered tables\n",
    "            out_filterb=out_filterb[:-1]\n",
    "            collect_filtered_files(out_filterb, collected_name )\n",
    "\n",
    "        print(\"read_filtered...\")\n",
    "        #read data and preprocess\n",
    "        tabledf=read_filtered_json(os.path.abspath(os.path.join(out_filterb, collected_name)))\n",
    "        result_samples=preprocess_page(tabledf)\n",
    "\n",
    "        #Array to randomly select tables\n",
    "        random.seed(random_buckets[index])\n",
    "        random_tables=random.sample(range(len(result_samples)),tabprobucket)\n",
    "\n",
    "        print(\"extract tables...\")\n",
    "        #list of df containing random tables of that bucket\n",
    "        rt_list=[result_samples[i] for i in random_tables]\n",
    "\n",
    "        #collect selected tables\n",
    "        rs_listlist.append(rt_list)\n",
    "        rs_list.extend(rt_list)\n",
    "\n",
    "        print(\"cleaning up...\")\n",
    "        file_path =os.path.join(out_filterb, collected_name)\n",
    "\n",
    "\n",
    "        if os.path.isfile(file_path):\n",
    "            # Verifies CSV file was created, then deletes unneeded files.\n",
    "            for clean_up in glob.glob(os.path.join(out_filterb,\"*.*\")):\n",
    "                #print(clean_up)\n",
    "                if not clean_up.startswith(os.path.join(out_filterb, collected_name)):#r\"C:\\Users\\johan\\Documents\\Studium\\Masterarbeit\\Code\\out_filter\\11\\result_bucket\"):    \n",
    "                    os.remove(clean_up)\n",
    "\n",
    "\n",
    "\n",
    "        pkl.dump(rs_list, open(str(pklname)+\".pkl\", \"wb\"))\n",
    "        print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "seeing-audit",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### generate new random sample for classifier evaluation \n",
    "#use tabprobucket=10 to recreate the original classifier sample\n",
    "create_sample(  0, 25,random_buckets, files, path, \"classifier_sample0\" , 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12427d9-0209-4e07-a442-b0720e56f0d3",
   "metadata": {},
   "source": [
    "If memory is not sufficient to generate the sample in one go, use the parameters begin and end of the create_sample function to iteratively create the sample.\n",
    "Use the cell below to combine the segments of the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "experimental-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all=[]\n",
    "for i in range(0,5):\n",
    "    ds=pkl.load(open(r\".\\classifier_sample\"+str(i)+\".pkl\", \"rb\"))\n",
    "    ds_all.extend(ds)\n",
    "pkl.dump(ds_all, open(r\".\\classifier_sample_complete.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas_env",
   "language": "python",
   "name": "geopandas_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
