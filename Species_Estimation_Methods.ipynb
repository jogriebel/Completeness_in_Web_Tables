{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "altered-sweet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johan\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\johan\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\johan\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import os\n",
    "from datetime import date, datetime, timedelta\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import argparse\n",
    "import math\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "advanced-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########gefilterte Datei einlesen#################\n",
    "#read the json file as a list of dicts\n",
    "path_json=\"\" #your path to the filtered json-file\n",
    "with open(path_json) as json_file:\n",
    "    data=json.load(json_file)\n",
    "\n",
    "#convert the json-file to a dataframe\n",
    "tabledf_22=pd.DataFrame.from_dict(data, orient=\"columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-garage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schätzen für eine Tabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absolute-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_page(tabledf_page):\n",
    "    #drop unneccessary columns\n",
    "    tabledf_page=tabledf_page.drop(columns=['pageID', 'content', 'contentHash', 'revisionId','similarityLast',  'comment', 'position','user', 'caption'])\n",
    "    \n",
    "    #ValidFrom als datetime Format\n",
    "    tabledf_page[\"validFrom\"]=pd.to_datetime(tabledf_page[\"validFrom\"], format='%Y-%m-%dT%H:%M:%SZ')\n",
    "    tabledf_page[\"timestamp_c\"]=tabledf_page.validFrom.transform(lambda x: x-x.min())\n",
    "    \n",
    "    #generate seperate df for each table on that page\n",
    "    grouped=tabledf_page.groupby(\"key\")\n",
    "    result=[g[1]for g in list(grouped)[:]]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "logical-suffering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fi's\n",
    "def calc_fi(changerows):\n",
    "    changedesc=list(reversed(changerows))\n",
    "    fi=[abs(changedesc[i+1]-changedesc[i]) for i in range(len(changedesc)-1)]\n",
    "    #die letzte Differenz ist das gesamte letzte Element (alle Zeilen sind neu)\n",
    "    fi.append(changedesc[len(changedesc)-1])\n",
    "    fi=list(fi)\n",
    "    return fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assured-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=sumi*fi\n",
    "def calc_n(i, fi, changerows):\n",
    "    n=sum([i*observations for i,observations in zip(i,fi)])\n",
    "\n",
    "    #Check\n",
    "  #  if n != sum(changerows):\n",
    "   #     print(\"Something went wrong with n!\", \"n:\",n, \"sum changerows: \", sum(changerows))\n",
    "    #    print(\"fi: \",fi)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cubic-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate all required variables\n",
    "def calculate_variables_N2(changerows):\n",
    "    D=max(changerows)\n",
    "    i=list(range(1,len(changerows)+1))\n",
    "    iminus1=list(range(0,len(changerows)))\n",
    "    fi=calc_fi(changerows)\n",
    "    f_1=fi[0]#np.sort(changerows)[-2:][1]-np.sort(changerows)[-2:][0]\n",
    "    n=calc_n(i,fi,changerows)\n",
    "    C=1-(fi[0]/n)\n",
    "    N1=D/C\n",
    "    gamma2=max(N1*sum([i*iminus1*fi for i,iminus1,fi in zip(i,iminus1,fi)])/(n*(n-1)-1),0)\n",
    "   # print(\"fi: \",fi, \"N1: \", N1, \"n: \",n)\n",
    "    return f_1, gamma2, N1, D, C,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sufficient-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N2\n",
    "def N2_estimation(C,n,gamma2, N1):\n",
    "    N2= N1 +(n*(1-C))/(C)*gamma2\n",
    "    if gamma2==0:\n",
    "        if N2!=N1:\n",
    "            print(\"N1 and N2 should be equal, but are not!\", \"N1: \",N1, \"N2: \",N2)\n",
    "\n",
    "    return N2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "raising-british",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Chao92)\n",
    "def estimate_N2_allinclusive(changerows):\n",
    "    iterative_estimates=[]\n",
    "    f_1_array=[]\n",
    "    f_1_array.append(changerows[0])\n",
    "\n",
    "    for e in range(1,len(changerows)):\n",
    "        changerows_i=changerows[0:e+1]\n",
    "        f_1, gamma2, N1, D, C,n=calculate_variables_N2(changerows_i)\n",
    "        N2=N2_estimation(C,n,gamma2, N1)\n",
    "        iterative_estimates.append(N2)\n",
    "        f_1_array.insert(0,f_1)\n",
    "\n",
    "    return iterative_estimates, f_1_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thousand-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single Outlier Reduction (Trushkowsky et al.)\n",
    "def calc_mu_SOR(changerows, fi):\n",
    "    \n",
    "    #avoid dividing by zero\n",
    "    if (len(changerows)-1)<1:\n",
    "        den=1\n",
    "    else:\n",
    "        den=(len(changerows)-1)\n",
    "    mu_SOR=sum([x /den  for x in fi])\n",
    "    return mu_SOR\n",
    "\n",
    "def calc_sigma_SOR(changerows, mu_SOR, fi):\n",
    "    if (len(changerows)-2)<1:\n",
    "        den=1\n",
    "    else:\n",
    "        den=(len(changerows)-2)\n",
    "    sigma_SOR=math.sqrt(sum([(x-mu_SOR)**2/den for x in fi]))\n",
    "    return sigma_SOR\n",
    "    \n",
    "def calc_f1_SOR(changerows, sigma_SOR, mu_SOR, f1):\n",
    "    f1_SOR=min(f1, 2*sigma_SOR+mu_SOR)\n",
    "    return f1_SOR\n",
    "\n",
    "def calc_variables_SOR(changerows):\n",
    "    fi=calc_fi(changerows)\n",
    "    f1=fi[0]\n",
    "    mu_SOR=calc_mu_SOR(changerows, fi)\n",
    "    sigma_SOR=calc_sigma_SOR(changerows, mu_SOR, fi)\n",
    "    f1_SOR=calc_f1_SOR(changerows, sigma_SOR, mu_SOR, f1)\n",
    "    i=list(range(1,len(changerows)+1))\n",
    "    n=calc_n(i, fi, changerows)\n",
    "    D=max(changerows)\n",
    "    return fi, f1, mu_SOR, sigma_SOR, f1_SOR,n, D\n",
    "\n",
    "def calc_SOR(f1_SOR, n, D):\n",
    "    N_SOR=D/(1-(f1_SOR/n))\n",
    "    return N_SOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "monetary-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimation(Single Outlier Reduction (SOR))\n",
    "\n",
    "def estimate_SOR_allinclusive(changerows):\n",
    "    iterative_estimates_SOR=[]\n",
    "    f_1_array_SOR=[]\n",
    "    f_1_array_SOR.append(changerows[0])\n",
    "\n",
    "    for e in range(1,len(changerows)):\n",
    "        changerows_i=changerows[0:e+1]\n",
    "        fi, f1, mu_SOR, sigma_SOR, f1_SOR,n, D=calc_variables_SOR(changerows_i)\n",
    "        N_SOR=calc_SOR(f1_SOR, n,D)\n",
    "        iterative_estimates_SOR.append(N_SOR)\n",
    "        f_1_array_SOR.insert(0,f1_SOR)\n",
    "\n",
    "    return iterative_estimates_SOR, f_1_array_SOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "desperate-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convergence metric rho (Luggen et al.)\n",
    "\n",
    "def convergence_rho(w, current, est):\n",
    "    N2_eval=current.iloc[len(current)-w:][est].values\n",
    "\n",
    "    D_eval=current.iloc[len(current)-w:]['rows']\n",
    "\n",
    "    \n",
    "    upper=sum(abs(N2_eval-D_eval)/D_eval)\n",
    "    rho=upper/w\n",
    "    return rho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "altered-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N1-UNIF (siehe Luggen et al.)\n",
    "def calc_N1UNIF(changerows):\n",
    "    fi=calc_fi(changerows)\n",
    "    f1=fi[0]\n",
    "    n=max(changerows)\n",
    "    S=1-(f1/n)\n",
    "    D=max(changerows)\n",
    "    N1_UNIF=D/S\n",
    "    \n",
    "    return N1_UNIF\n",
    "\n",
    "def estimate_N1UNIF(changerows):\n",
    "    iterative_estimates_N1UNIF=[]\n",
    "\n",
    "    for e in range(1,len(changerows)):\n",
    "        changerows_i=changerows[0:e+1]\n",
    "        N1_UNIF=calc_N1UNIF(changerows_i)\n",
    "        iterative_estimates_N1UNIF.append(N1_UNIF)\n",
    "\n",
    "    return iterative_estimates_N1UNIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "latter-diving",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jack1 (siehe Luggen et al.)\n",
    "\n",
    "def calc_jack(changerows,e):\n",
    "    D=max(changerows)\n",
    "    k=e\n",
    "    fi=calc_fi(changerows)\n",
    "    f1=fi[0]\n",
    "    \n",
    "    N_jack1=D+(k-1)/(k)*f1\n",
    "    \n",
    "    if k-1 <1:\n",
    "        N_jack2=0\n",
    "    else:\n",
    "        N_jack2=D + (2*k-3)/(k)*f1 -((k-2)**2/(k*(k-1))*fi[1])\n",
    "    \n",
    "    return N_jack1, N_jack2\n",
    "\n",
    "def estimate_Jack(changerows):\n",
    "    iterative_estimates_Jack1=[]\n",
    "    iterative_estimates_Jack2=[]\n",
    "    \n",
    "    for e in range(1,len(changerows)):\n",
    "        changerows_i=changerows[0:e+1]\n",
    "        N_jack1, N_jack2=calc_jack(changerows_i, e)\n",
    "        iterative_estimates_Jack1.append(N_jack1)\n",
    "        iterative_estimates_Jack2.append(N_jack2)\n",
    "    \n",
    "    return iterative_estimates_Jack1, iterative_estimates_Jack2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=preprocess_page(tabledf_22) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "scientific-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified: for loop is now outside of the function\n",
    "\n",
    "def estimate_caprecap(current):\n",
    "\n",
    "    changerows=list(current['rows'])\n",
    "\n",
    "    #add row numbers that start from 1 per table (are used as sample numbers for graphics)\n",
    "    current['samples'] = np.arange(len(current))+1\n",
    "\n",
    "    iterative_estimates, f_1_array=estimate_N2_allinclusive(changerows)\n",
    "\n",
    "    #SOR\n",
    "    iterative_estimates_SOR, f_1_array_SOR=estimate_SOR_allinclusive(changerows)\n",
    "\n",
    "    #add to current dataframe\n",
    "    iterative_estimates.insert(0,changerows[0])\n",
    "    current['N2_allrows']=iterative_estimates\n",
    "    current['f_1']=list(reversed(f_1_array))\n",
    "\n",
    "    #Single Outlier Reduction (SOR)\n",
    "    iterative_estimates_SOR.insert(0,changerows[0]) #first sample doesnt make sense to estimate\n",
    "    current['N_SOR']=iterative_estimates_SOR\n",
    "    current['f1_SOR']=list(reversed(f_1_array_SOR))\n",
    "\n",
    "    #N1-UNIF (Sample Coverage and the Good-Turing Estimator)\n",
    "    iterative_estimates_N1UNIF=estimate_N1UNIF(changerows)\n",
    "    iterative_estimates_N1UNIF.insert(0,changerows[0]) #first sample doesnt make sense to estimate 0\n",
    "    current['N1_UNIF']=iterative_estimates_N1UNIF\n",
    "\n",
    "    #Jackknife Estimators\n",
    "    iterative_estimates_Jack1, iterative_estimates_Jack2=estimate_Jack(changerows)\n",
    "    iterative_estimates_Jack1.insert(0,changerows[0]) #first sample doesnt make sense to estimate\n",
    "    iterative_estimates_Jack2.insert(0,changerows[0]) #first sample doesnt make sense to estimate\n",
    "    current['Jack1']=iterative_estimates_Jack1\n",
    "    current['Jack2']=iterative_estimates_Jack2\n",
    "    \n",
    "    return current\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimation for the entire page: without sample periods, alle Einträge werden in Betracht gezogen (kein rows.unique())\n",
    "\n",
    "\n",
    "def estimate_species(result):\n",
    "\n",
    "    for t in range(len(result)-1):\n",
    "        current=pd.DataFrame(result[t])\n",
    "        #sample1=current[(current['timestamp_c']-current['timestamp_c'].min()<=sp1)]\n",
    "        #mit allen Einträgen\n",
    "        changerows=list(current['rows'])#.unique()\n",
    "\n",
    "\n",
    "        #add row numbers that start from 1 per table (are used as sample numbers for graphics)\n",
    "        current['samples'] = np.arange(len(current))+1\n",
    "\n",
    "        iterative_estimates, f_1_array=estimate_N2_allinclusive(changerows)\n",
    "\n",
    "        #SOR\n",
    "        iterative_estimates_SOR, f_1_array_SOR=estimate_SOR_allinclusive(changerows)\n",
    "\n",
    "        #add to current dataframe\n",
    "        iterative_estimates.insert(0,changerows[0])\n",
    "        current['N2_allrows']=iterative_estimates\n",
    "        current['f_1']=list(reversed(f_1_array))\n",
    "\n",
    "\n",
    "\n",
    "        #Single Outlier Reduction (SOR)\n",
    "        iterative_estimates_SOR.insert(0,changerows[0]) #first sample doesnt make sense to estimate\n",
    "        current['N_SOR']=iterative_estimates_SOR\n",
    "        current['f1_SOR']=list(reversed(f_1_array_SOR))\n",
    "\n",
    "\n",
    "        #N1-UNIF (Sample Coverage and the Good-Turing Estimator)\n",
    "        iterative_estimates_N1UNIF=estimate_N1UNIF(changerows)\n",
    "        iterative_estimates_N1UNIF.insert(0,changerows[0]) #first sample doesnt make sense to estimate 0\n",
    "        current['N1_UNIF']=iterative_estimates_N1UNIF\n",
    "\n",
    "\n",
    "        #Jackknife Estimators\n",
    "        iterative_estimates_Jack1, iterative_estimates_Jack2=estimate_Jack(changerows)\n",
    "        iterative_estimates_Jack1.insert(0,changerows[0]) #first sample doesnt make sense to estimate\n",
    "        iterative_estimates_Jack2.insert(0,changerows[0]) #first sample doesnt make sense to estimate\n",
    "        current['Jack1']=iterative_estimates_Jack1\n",
    "        current['Jack2']=iterative_estimates_Jack2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #add current dataframe back to all the dataframes\n",
    "        #braucht es das überhaupt?\n",
    "        result[t]=current\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimation\n",
    "result=estimate_species(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Evaluation: Convergence Metric#############\n",
    "#variables to set the scope of tables for which the convergence metric and the graphics are done\n",
    "begin=0\n",
    "end=len(result)-1\n",
    "w=4 #considered samples for rho#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "placed-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########Convergence Metric########################\n",
    "estimates=[ \"N2_allrows\", \"N_SOR\", \"N1_UNIF\", \"Jack1\", \"Jack2\"]\n",
    "estimates_rho=[ \"rho_N2_allrows\", \"rho_N_SOR\", \"rho_N1_UNIF\", \"rho_Jack1\", \"rho_Jack2\"]\n",
    "\n",
    "def convergence_complete(result, estimates, estimates_rho):\n",
    "    \n",
    "    #create dict of lists to collect the results of all estimates\n",
    "    dfEvaluate=pd.DataFrame(columns=estimates_rho)\n",
    "\n",
    "    for t in range(begin,end):#len(result)-1): #iteration over tables\n",
    "        dfEvaluate=pd.DataFrame(result[t])\n",
    "\n",
    "        estimates_rho=estimates_rho#[ \"rho_N2_allrows\", \"rho_N_SOR\", \"rho_N1_UNIF\", \"rho_Jack1\", \"rho_Jack2\"]\n",
    "        rho_dict={}\n",
    "\n",
    "        for rest in estimates_rho:\n",
    "            #set the first three entries to 0 to have the same length as the df, w takes the last 4 samples into account\n",
    "           # rho_dict[rest]=[0]*(w-1) \n",
    "            rho_dict[rest]=[]##\n",
    "\n",
    "            #dfEvaluate[rest]=\"\"\n",
    "\n",
    "        ##for l in range(w, len(dfEvaluate)+1): #iteration over rows in table\n",
    "        for l in range(0, len(dfEvaluate)):  \n",
    "            current=dfEvaluate[0:l]\n",
    "\n",
    "\n",
    "            for est in range(len(estimates)): #iteration over different estimates; hier kein -1, da sonst der letzte Estimate nicht beachtet wird bei der Schätzung\n",
    "                #Convergence-Fehler für alle Schätzer berechnen           \n",
    "                if len(current)<w:\n",
    "                    rho=np.nan\n",
    "                else:\n",
    "                    rho=convergence_rho(w, current, estimates[est])\n",
    "                rho_dict[estimates_rho[est]].append(rho)\n",
    "\n",
    "\n",
    "\n",
    "        #und als Spalten in df hinzufügen\n",
    "        df_rho=pd.DataFrame.from_dict(rho_dict, orient='index').transpose() # in df_rho ist alles noch enthalten\n",
    "\n",
    "        #df_rho[\"samples\"]=dfEvaluate[\"samples\"]\n",
    "\n",
    "        if pd.Series(estimates_rho).isin(dfEvaluate.columns).all(): \n",
    "            #damit die Spalten nicht immer wieder hinzugefügt werden\n",
    "            for est_rho in estimates_rho:\n",
    "                dfEvaluate[est_rho]=df_rho[est_rho].values #.values if the indexes aren't matching (important here!!)\n",
    "\n",
    "        else:\n",
    "            #print(\"Rho columns are not in the dataframe.\")\n",
    "            df_rho.index=dfEvaluate.index\n",
    "            dfEvaluate = pd.concat([dfEvaluate, df_rho], axis=1)\n",
    "\n",
    "        result[t]=dfEvaluate\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=convergence_complete(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save this to csv###\n",
    "pkl.dump(result, open(\"Species_iterative_estimation_pkl.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "###From Luggen-Paper####\n",
    "\n",
    "def conv(estimates, distincts):\n",
    "    # window size\n",
    "    w = 4\n",
    "    #remove infinite values\n",
    "    estimates = list(filter(lambda x: x != float('Inf'), estimates))\n",
    "    n = len(estimates) \n",
    "    # bail out if not enough samples\n",
    "    if n < w:\n",
    "        return 1 \n",
    "    s = [ (abs(estimates[i] - distincts[i]) / distincts[i]) for i in range(n-w, n)] #again, mind the indices.\n",
    "\n",
    "    return sum(s) / w\n",
    "\n",
    "\n",
    "def plot(df, title, ax, idx):\n",
    "    lines = ['N2_allrows', 'N_SOR', 'N1_UNIF', 'Jack1', 'Jack2','rows']\n",
    "\n",
    "    ax.set_title(title + '\\n')    \n",
    "\n",
    "    #plot estimators\n",
    "    resrow = {}\n",
    "    metrics = [] \n",
    "\n",
    "    for idx, column in enumerate(lines):\n",
    "        sns.lineplot(x=df[\"samples\"], y=column, data=df, label=column,  ax=ax,  markers=idx, errorbar=None)\n",
    "\n",
    "    ax.set(xlabel='Samples', ylabel='Cardinality')\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "    return df\n",
    "\n",
    "def plotInd(df, title, ax, idx):\n",
    "    # indicators\n",
    "    #normieren auf [0,1]\n",
    "    df['f_1_normiert'] = df['f_1']/df['rows'].max()#.astype(np.float64)\n",
    "    df['Distinct'] = df['rows']/df['rows'].max()#.astype(np.float64)\n",
    "    df['f1_SOR_normiert']=df['f1_SOR']/df['rows'].max()\n",
    "\n",
    "\n",
    "    for idx, column in enumerate(['f_1_normiert','Distinct','f1_SOR_normiert' ]):\n",
    "        sns.lineplot(x=df[\"samples\"], y=column, data=df, label=column, ax=ax, errorbar=None)\n",
    "\n",
    "    ax.set(xlabel='Samples', ylabel='Indicators')\n",
    "    sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "    \n",
    "def plotConvergence(df, title, ax, idx):\n",
    "    \n",
    "    lines = [\"rho_N2_allrows\", \"rho_N_SOR\", \"rho_N1_UNIF\", \"rho_Jack1\"]\n",
    "\n",
    "    #ax.set_title(title + '\\n')    \n",
    "    #plot estimators\n",
    "    #resrow = {}\n",
    "    #metrics = [] \n",
    "    \n",
    "    for idx, column in enumerate(lines):\n",
    "        #print(\"idx: \", idx, \"column: \",column)\n",
    "        if df[column].isnull().all():\n",
    "            print(\"No rho for this row.\")\n",
    "        else:\n",
    "            df = df[~np.isnan(df[column])]\n",
    "            sns.lineplot(x=df[\"samples\"], y=df[column].dropna(), data=df.dropna(), label=column,  ax=ax,  markers=idx, errorbar=None)\n",
    "        #print(\"y-value:\", df[column])\n",
    "\n",
    "            ax.set(xlabel='Samples', ylabel='Convergence')\n",
    "            sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "    #return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################Grafiken#####################\n",
    "for t in range(begin,10):#len(result)-1):\n",
    "\n",
    "        dfEstimate = pd.DataFrame(result[t], columns=[\"pageTitle\", \"validFrom\", \"rows\", \"headings\", \"contentType\", \"key\", \"timestamp_c\", \"N2_allrows\", \"f_1\", \"samples\", \"f1_SOR\", \"N_SOR\", \"N1_UNIF\", \"Jack1\", \"Jack2\", \"rho_N2_allrows\", \"rho_N_SOR\", \"rho_N1_UNIF\", \"rho_Jack1\", \"rho_Jack2\"])\n",
    "        f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=[8, 4])\n",
    "        \n",
    "\n",
    "        graphic_result = plot(dfEstimate, str(dfEstimate[\"pageTitle\"].values[0]) +\": \" + str(dfEstimate[\"headings\"].values[0]) + \" (\" + str(dfEstimate[\"key\"].values[0]) + \")\", ax1, 0) \n",
    "        plotInd(dfEstimate, str(dfEstimate[\"pageTitle\"].values[0]) + \" (\" + str(dfEstimate[\"key\"].values[0]) + \")\", ax2, 0)\n",
    "        plotConvergence(dfEstimate, str(dfEstimate[\"pageTitle\"].values[0]) +\": \" + str(dfEstimate[\"headings\"].values[0]) + \" (\" + str(dfEstimate[\"key\"].values[0]) + \")\", ax3, 0)\n",
    "        \n",
    "        #plt.savefig(r\"\\graphics\\\\\"  +str(dfEstimate[\"key\"].values[0]) + '.pdf')\n",
    "        #logging.info( \"saved \" + str(estimtableate) + '.pdf' )\n",
    "        #plt.close()\n",
    "        plt.show()\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-hayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rho pro Tabelle gemittelt\n",
    "\n",
    "estimates_rho=[ \"rho_N2_allrows\", \"rho_N_SOR\", \"rho_N1_UNIF\", \"rho_Jack1\", \"rho_Jack2\"]\n",
    "\n",
    "\n",
    "def calc_rho_meanpertable(begin, end, result, estimates_rho):\n",
    "    columns=[\"key\", \"caption\"]\n",
    "    columns.extend(estimates_rho)\n",
    "    print(columns)\n",
    "    df_rhomean=pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for r in range(begin,end):#len(result)-1):\n",
    "\n",
    "\n",
    "        current=pd.DataFrame(result[r])\n",
    "\n",
    "        rho_mean={}\n",
    "        rho_mean[\"key\"]=current[\"key\"].values[0]\n",
    "        rho_mean[\"caption\"]= str(current[\"pageTitle\"].values[0])+ \": \"+ str(current[\"headings\"].values[0])\n",
    "\n",
    "\n",
    "        for rest in range(len(estimates_rho)): #ohne -1 sonst wird das letzte rho nicht beachtet\n",
    "            mean=current[estimates_rho[rest]].mean()\n",
    "            rho_mean[estimates_rho[rest]]=mean\n",
    "\n",
    "\n",
    "        df_rhomean=df_rhomean.append(rho_mean, ignore_index=True)\n",
    "    return df_rhomean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "###über Tabellen hinweg pro Schätzer gemittelt\n",
    "def rho_mean_total(estimates_rho, df_rhomean):\n",
    "    means=[]\n",
    "    for mean in estimates_rho:\n",
    "        m=df_rhomean[mean].mean()\n",
    "        means.append(m)\n",
    "\n",
    "    df_rhomean_mean=pd.DataFrame( columns=[ \"rho_N2_allrows\", \"rho_N_SOR\", \"rho_N1_UNIF\", \"rho_Jack1\", \"rho_Jack2\"])\n",
    "    df_rhomean_mean.loc[0]=means\n",
    "    return df_rhomean_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rhomean=calc_rho_meanpertable(begin, end, result, estimates_rho)\n",
    "df_rhomean_mean=rho_mean_total(estimates_rho, df_rhomean)\n",
    "df_rhomean_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "########Sample Periods################################\n",
    "\n",
    "def create_sampled_tables(result_sample, period):\n",
    "    if \"result\" in locals():\n",
    "        print(\"result is being copied\")\n",
    "        result_sample=result.copy() #Kopie, damit das andere result Objekt, das nicht in samples aufgeteilt ist, weiterbesteht\n",
    "    \n",
    "    for table in range(len(result_sample)-1):\n",
    "\n",
    "        df=pd.DataFrame(result_sample[table])\n",
    "\n",
    "        dates=pd.to_datetime(df.validFrom, format='%Y-%m-%dT%H:%M:%SZ', errors=\"coerce\") #str2time(df.validFrom).reset_index()#\n",
    "        df=df.assign(date=dates)\n",
    "\n",
    "       #this return only the date and the rows column     \n",
    "       #! dfg = pd.DataFrame(df.assign(date=dates)\n",
    "         #!   .groupby([pd.Grouper(key='date',  freq='3Y', origin=pd.Timestamp(min(df[\"validFrom\"].values)))])[\"rows\"]\n",
    "           #!.max())\n",
    "            #.reset_index())\n",
    "    \n",
    "        #this doesn't return the correct date column with the boundries of the sample periods\n",
    "        dfg=df.loc[df.assign(date=dates).groupby([pd.Grouper(key='date',  freq=period, origin=pd.Timestamp(min(df[\"validFrom\"].values)))])['rows'].agg(\n",
    "    lambda x : np.nan if x.count() == 0 else x.idxmax()).dropna()]\n",
    "        dfg=dfg.reset_index()\n",
    "         \n",
    "       # dfg=df.loc[df.assign(date=dates).groupby([pd.Grouper(key='date',  freq='3Y', origin=\"start\")])['rows'].idxmax()].reset_index(drop=True)\n",
    "        result_sample[table]=dfg\n",
    "    \n",
    "    return result_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-skiing",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sample=preprocess_page(tabledf_22) \n",
    "result_sample=create_sampled_tables(result_sample, \"1Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-stomach",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Estimate with Sample Periods######\n",
    "#estimation\n",
    "result_sample=estimate_species(result_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Convergence Metrics with Sample Periods######\n",
    "result_sample=convergence_complete(result_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####PLots#####\n",
    "def plot_all(result, begin, end):\n",
    "    for t in range(begin,end):#len(result)-1):\n",
    "\n",
    "            dfEstimate = pd.DataFrame(result[t], columns=[\"pageTitle\", \"validFrom\", \"rows\", \"headings\", \"contentType\", \"key\", \"timestamp_c\", \"N2_allrows\", \"f_1\", \"samples\", \"f1_SOR\", \"N_SOR\", \"N1_UNIF\", \"Jack1\", \"Jack2\", \"rho_N2_allrows\", \"rho_N_SOR\", \"rho_N1_UNIF\", \"rho_Jack1\", \"rho_Jack2\"])\n",
    "            f, (ax1, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=[8, 4])\n",
    "\n",
    "\n",
    "            graphic_result = plot(dfEstimate, str(dfEstimate[\"pageTitle\"].values[0]) +\": \" + str(dfEstimate[\"headings\"].values[0]) + \" (\" + str(dfEstimate[\"key\"].values[0]) + \")\", ax1, 0) \n",
    "            plotInd(dfEstimate, str(dfEstimate[\"pageTitle\"].values[0]) + \" (\" + str(dfEstimate[\"key\"].values[0]) + \")\", ax2, 0)\n",
    "            plotConvergence(dfEstimate, str(dfEstimate[\"pageTitle\"].values[0]) +\": \" + str(dfEstimate[\"headings\"].values[0]) + \" (\" + str(dfEstimate[\"key\"].values[0]) + \")\", ax3, 0)\n",
    "\n",
    "            plt.savefig(r\"\\graphics\\\\\"  +str(dfEstimate[\"key\"].values[0]) + '.pdf')\n",
    "            #plt.savefig(args.outpath + 'figures/' +str(table) + '.png')\n",
    "            #logging.info( \"saved \" + str(estimtableate) + '.pdf' )\n",
    "            #plt.close()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all(result_sample, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rho_samples=calc_rho_meanpertable(0, len(result_sample)-1, result_sample, estimates_rho)\n",
    "mean_rho_samples=rho_mean_total(estimates_rho, df_rho_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rho_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rhomean_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-theorem",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
